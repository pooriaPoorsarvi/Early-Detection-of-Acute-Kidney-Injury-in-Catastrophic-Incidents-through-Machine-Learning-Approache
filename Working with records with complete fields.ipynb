{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import missingno as msno\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from keras.activations import *\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from functools import partial\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from IPython.core.magic import (register_line_magic,\n",
    "                                register_cell_magic)\n",
    "\n",
    "import gplearn\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from gplearn.genetic import SymbolicTransformer\n",
    "from gplearn.functions import make_function\n",
    "from gplearn.fitness import make_fitness\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vars():\n",
    "    \n",
    "    global df\n",
    "    global df1\n",
    "    global df2\n",
    "    \n",
    "    df = pd.read_csv(\"fscore.csv\")\n",
    "    df = df[['cpk_1', 'ldh_1', 'uricac_1', 'k_1', \"crat3f\", \"newarf\"]]\n",
    "    \n",
    "    matrix = df.values\n",
    "    for i, iv in enumerate(matrix):\n",
    "        for j, jv in enumerate(iv):\n",
    "            if jv.strip() == \"\":\n",
    "                if j == matrix.shape[1] - 1 :\n",
    "                    matrix[i, j] = 0\n",
    "                else:\n",
    "                    matrix[i, j] = np.nan\n",
    "                    \n",
    "    matrix = matrix.astype(np.float32)\n",
    "    df = pd.DataFrame(matrix, columns=df.columns)\n",
    "    df1 = df[['cpk_1', 'ldh_1', 'uricac_1', 'k_1', \"newarf\"]]\n",
    "    df2 = df[[\"crat3f\", \"newarf\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_pred_cret(shape):\n",
    "    '''\n",
    "    this model predicts crat3f\n",
    "    '''\n",
    "    in1 = Input(shape)\n",
    "    X = Dense(1, activation=linear)(in1)\n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_for_cret(shape):\n",
    "    '''\n",
    "    this model predicts arf from crat3f\n",
    "    '''\n",
    "    in1 = Input(shape)\n",
    "    X = Dense(5, activation=linear)(in1)\n",
    "    X =  PReLU()(X)\n",
    "    X = Dense(1, activation=sigmoid)(X)\n",
    "    \n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_true, y_pred):\n",
    "    \n",
    "    '''\n",
    "    gives rec0 and rec1\n",
    "    '''\n",
    "    \n",
    "    arr = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    rec0 = arr[0][0] / (arr[0][0]+arr[0][1])\n",
    "    rec1 = arr[1][1] / (arr[1][1]+arr[1][0])\n",
    "    return rec0, rec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fit_acc(y, y_pred, w):\n",
    "    \n",
    "    '''\n",
    "    fitness function for the genetic programming part of the paper\n",
    "    '''\n",
    "    \n",
    "    y_pred = y_pred.astype(np.float32)\n",
    "    y_pred = (y_pred >= 0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    acc = np.sum((y_pred == y)) / y_pred.shape[0]\n",
    "    #     we also could have used the following for fitness\n",
    "    #     rec0, rec1 = get_acc(y, y_pred)\n",
    "    #     res = acc + rec1 * 1.1\n",
    "    res = acc\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_acc = make_fitness(_fit_acc, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_and_clean(data_frame, k):\n",
    "    '''\n",
    "    this function is used for making our k fold simpler \n",
    "    it receives the data and drops the NaN part of the data set\n",
    "    also the shape of the data set and the size that each test set should be is computed \n",
    "    ind is later used to shuffle our data\n",
    "    '''\n",
    "    \n",
    "    data_frame = data_frame.dropna()\n",
    "    shape0 = data_frame.shape[0]\n",
    "    ind = np.arange(shape0)\n",
    "    size = math.ceil(shape0 / k)\n",
    "    \n",
    "    \n",
    "    return data_frame, shape0, ind, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure_1(data, train, scaler = None, model = None):\n",
    "    '''\n",
    "    training procedure : we use one neural network (one shallow neural network) to predict the state of a patient using \n",
    "    the predicted or real value of crat3f\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    X = data[:, :-1]\n",
    "    if train :\n",
    "        scaler  = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    if train:\n",
    "        model = get_model_for_cret([1])\n",
    "        model.compile(keras.optimizers.adam(5e-4), loss=keras.losses.binary_crossentropy, metrics=[\"accuracy\"])\n",
    "        model.fit(X, y, epochs=700, batch_size=8, verbose = 0)\n",
    "    y_p = model.predict(X) >= 0.5\n",
    "    rec0, rec1 = get_acc(y, y_p)\n",
    "    \n",
    "    return rec0, rec1, scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure_2(data, train, scaler = None, model = None):\n",
    "    '''\n",
    "    training procedure : uses genetic programming and data from the first four columns \n",
    "    to find the state of the patient without crat3f\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    X = data[:, :-1]\n",
    "    if train :\n",
    "        scaler  = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    if train:\n",
    "        model = SymbolicRegressor(function_set=['add', 'sub', 'mul', 'div', \"max\", \"log\", \"sqrt\", \"abs\"], \n",
    "                               metric = fit_acc, verbose = 0,\n",
    "                          population_size=1000, generations=20,\n",
    "                          stopping_criteria = 3, random_state = 42, init_depth = (5,15), \n",
    "                               tournament_size = 100, const_range=(-1.5, 1.5),p_crossover = 0.85 ,p_subtree_mutation = 0.01,\n",
    "                              p_hoist_mutation = 0.04, p_point_mutation=0.1, p_point_replace=0.1)\n",
    "        \n",
    "        model.fit(X, y)\n",
    "    y_p = model.predict(X) >= 0\n",
    "    rec0, rec1 = get_acc(y, y_p)\n",
    "    \n",
    "    return rec0, rec1, scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure_3(data, train, scaler = None, model1 = None, model2 = None):\n",
    "    '''\n",
    "    training procedure : uses one neural network with two parts two predict the state of the patient\n",
    "    this function is only used on one data set\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    if train :\n",
    "        scaler  = StandardScaler().fit(data[:, :-1])\n",
    "    data[:, :-1] = scaler.transform(data[:, :-1])\n",
    "    X = data[:, :-2]\n",
    "    y = data[:, -2]\n",
    "    \n",
    "    if train:\n",
    "        model1 = get_model_for_pred_cret([data.shape[1] - 2])\n",
    "        model1.compile(keras.optimizers.adam(5e-4), loss=keras.losses.mean_squared_error)\n",
    "        model1.fit(X, y, epochs=1000, batch_size=8, verbose = 0)\n",
    "        \n",
    "    \n",
    "    y = data[:, -1]\n",
    "    X = model1.predict(X)\n",
    "    \n",
    "    if train :    \n",
    "        model2 = get_model_for_cret([1])\n",
    "        model2.compile(keras.optimizers.adam(1e-3), loss=keras.losses.binary_crossentropy, metrics=[\"accuracy\"])\n",
    "        model2.fit(X, y, epochs=1000, batch_size=8, verbose = 0)\n",
    "        \n",
    "        \n",
    "    y_p = model2.predict(X) >= 0.5\n",
    "    rec0, rec1 = get_acc(y, y_p)\n",
    "    \n",
    "    return rec0, rec1, scaler, model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure_4(data1, data2, train, scaler1 = None, scaler2 = None, model1 = None, model2 = None):\n",
    "    '''\n",
    "    training procedure : uses one neural network with two parts two predict the state of the patient\n",
    "    this function is first uses the first data set to learn how to regress the value of crat3f \n",
    "    and then it uses the second data set inorder to predict the state of the paitient\n",
    "    '''\n",
    "    data1 = data1.copy()\n",
    "    data2 = data2.copy()\n",
    "    \n",
    "    if(train):\n",
    "        scaler1 = StandardScaler().fit(data2[:, :-1])\n",
    "        scaler2 = StandardScaler().fit(data1[:, -2:-1])\n",
    "    \n",
    "    data1[:, :-2] = scaler1.transform(data1[:, :-2])\n",
    "    data2[:, :-1] = scaler1.transform(data2[:, :-1])\n",
    "    data1[:, -2:-1] = scaler2.transform(data1[:, -2:-1])\n",
    "    \n",
    "    X = data1[:, :-2]\n",
    "    y = data1[:, -2]\n",
    "    \n",
    "    if (train):\n",
    "        model1 = get_model_for_pred_cret([data1.shape[1] - 2])\n",
    "        model1.compile(keras.optimizers.adam(5e-4), loss=keras.losses.mean_squared_error)\n",
    "        model1.fit(X, y, epochs=1000, batch_size=8, verbose = 0)\n",
    "    \n",
    "    X = model1.predict(data2[:, :-1])\n",
    "    y = data2[:, -1]\n",
    "    \n",
    "    if (train) :    \n",
    "        model2 = get_model_for_cret([1])\n",
    "        model2.compile(keras.optimizers.adam(1e-3), loss=keras.losses.binary_crossentropy, metrics=[\"accuracy\"])\n",
    "        model2.fit(X, y, epochs=1000, batch_size=8, verbose = 0)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    y_p = model2.predict(X) >= 0.5\n",
    "    rec0, rec1 = get_acc(y, y_p)\n",
    "    \n",
    "    return rec0, rec1, scaler1, scaler2, model1, model2\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_procedure_5(data, train, scaler = None, model = None):\n",
    "    '''\n",
    "    training procedure : uses a neural network and data from the first four columns \n",
    "    to find the state of the patient without crat3f\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    X = data[:, :-1]\n",
    "    if train :\n",
    "        scaler  = StandardScaler().fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    if train:\n",
    "        model = get_model_for_cret([data.shape[1]-1])\n",
    "        model.compile(keras.optimizers.adam(5e-4), loss=keras.losses.binary_crossentropy, metrics=[\"accuracy\"])\n",
    "        model.fit(X, y, epochs = 1000, verbose = 0, batch_size = 8)\n",
    "    y_p = model.predict(X) >= 0.5\n",
    "    rec0, rec1 = get_acc(y, y_p)\n",
    "    \n",
    "    return rec0, rec1, scaler, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(model_to_use, k = 10):\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    get_vars()\n",
    "    global df\n",
    "    global df1\n",
    "    global df2\n",
    "    \n",
    "    \n",
    "    df, shape0df, ind, size = drop_and_clean(df, k)\n",
    "    df1, shape0df1, ind1, size1 = drop_and_clean(df1, k)\n",
    "    df2, shape0df2, ind2, size2 = drop_and_clean(df2, k)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if model_to_use == 0 :\n",
    "        np.random.shuffle(ind2)\n",
    "        data = df2.values\n",
    "        data = data[ind2]\n",
    "        function_to_use = training_procedure_1\n",
    "        sizef = size2\n",
    "        max_size = df2.shape[0]\n",
    "    \n",
    "    if model_to_use == 1 :\n",
    "        np.random.shuffle(ind)\n",
    "        data = df.values\n",
    "        data = data[ind]\n",
    "        function_to_use = training_procedure_3\n",
    "        sizef = size\n",
    "        max_size = df.shape[0]\n",
    "        \n",
    "    if model_to_use == 2 :\n",
    "        function_to_use = training_procedure_4\n",
    "        np.random.shuffle(ind)\n",
    "        data = df.values\n",
    "        data = data[ind]\n",
    "        sizef = size\n",
    "        max_size = df.shape[0]\n",
    "#         add : additional\n",
    "        np.random.shuffle(ind1)\n",
    "        data_add = df1.values \n",
    "        data_add = data_add[ind1]\n",
    "        sizef_add = size1\n",
    "        max_size_add = df1.shape[0]\n",
    "    if model_to_use in [3, 4] :\n",
    "        if model_to_use == 3 :\n",
    "            function_to_use = training_procedure_2\n",
    "        else :\n",
    "            function_to_use = training_procedure_5\n",
    "        np.random.shuffle(ind1)\n",
    "        data = df1.values \n",
    "        data = data[ind1]\n",
    "        sizef = size1\n",
    "        max_size = df1.shape[0]\n",
    "        \n",
    "    \n",
    "    trs = []\n",
    "    tss = []\n",
    "    mdls = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        print(\"k : \", i)\n",
    "        \n",
    "        start = sizef * i\n",
    "        end = min(max_size, (i + 1) * sizef)\n",
    "        \n",
    "        data_train = np.concatenate([data[:start], data[end:]], axis = 0)\n",
    "        data_test  = data[start:end]\n",
    "        \n",
    "        \n",
    "        if model_to_use == 0 :\n",
    "            \n",
    "        \n",
    "            rec0, rec1, scaler, model = function_to_use(data_train, True)\n",
    "            trs.append((rec0, rec1))\n",
    "            rec0, rec1, scaler, model = function_to_use(data_test, False, scaler, model)\n",
    "            tss.append((rec0, rec1))\n",
    "            mdls.append(model)\n",
    "            print(\"train\", trs[-1])\n",
    "            print(\"test\", tss[-1])\n",
    "            \n",
    "        if model_to_use == 1 :\n",
    "            \n",
    "            rec0, rec1, scaler, model1, model2 = function_to_use(data_train, True)\n",
    "            trs.append((rec0, rec1))\n",
    "            rec0, rec1, scaler, model1, model2 = function_to_use(data_test, False, scaler, model1, model2)\n",
    "            tss.append((rec0, rec1))\n",
    "            mdls.append((model1, model2))\n",
    "            print(\"train\", trs[-1])\n",
    "            print(\"test\", tss[-1])\n",
    "            \n",
    "            \n",
    "        if model_to_use == 2 :\n",
    "            \n",
    "            start_add = sizef_add * i\n",
    "            end_add = min(max_size_add, (i + 1) * sizef_add)\n",
    "\n",
    "            data_train_add = np.concatenate([data_add[:start_add], data_add[end_add:]], axis = 0)\n",
    "            data_test_add  = data_add[start_add:end_add]\n",
    "            \n",
    "            rec0, rec1, scaler1, scaler2, model1, model2 = function_to_use(data_train, data_train_add, True)\n",
    "            trs.append((rec0, rec1))\n",
    "            rec0, rec1, scaler1, scaler2, model1, model2 = function_to_use(data_test, data_test_add, False, scaler1, scaler2, model1, model2)\n",
    "            tss.append((rec0, rec1))\n",
    "            mdls.append((model1, model2))\n",
    "            print(\"train\", trs[-1])\n",
    "            print(\"test\", tss[-1])\n",
    "            \n",
    "        if model_to_use in [3, 4] :\n",
    "            \n",
    "            rec0, rec1, scaler, model = function_to_use(data_train, True)\n",
    "            trs.append((rec0, rec1))\n",
    "            rec0, rec1, scaler, model = function_to_use(data_test, False, scaler, model)\n",
    "            tss.append((rec0, rec1))\n",
    "            mdls.append(model)\n",
    "            print(\"train\", trs[-1])\n",
    "            print(\"test\", tss[-1])\n",
    "        \n",
    "    return trs, tss, mdls\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0\n",
      "k :  0\n",
      "train (0.9931972789115646, 0.9642857142857143)\n",
      "test (1.0, 1.0)\n",
      "k :  1\n",
      "train (0.9953917050691244, 0.9682539682539683)\n",
      "test (1.0, 1.0)\n",
      "k :  2\n",
      "train (0.9930555555555556, 0.9692307692307692)\n",
      "test (1.0, 1.0)\n",
      "k :  3\n",
      "train (0.9977064220183486, 0.9672131147540983)\n",
      "test (0.9591836734693877, 1.0)\n",
      "k :  4\n",
      "train (0.9954128440366973, 0.9836065573770492)\n",
      "test (1.0, 0.8571428571428571)\n",
      "k :  5\n",
      "train (0.9931506849315068, 0.9661016949152542)\n",
      "test (1.0, 1.0)\n",
      "k :  6\n",
      "train (0.9954337899543378, 0.9661016949152542)\n",
      "test (0.9787234042553191, 1.0)\n",
      "k :  7\n",
      "train (0.993103448275862, 0.967741935483871)\n",
      "test (1.0, 1.0)\n",
      "k :  8\n",
      "train (0.9930875576036866, 0.9841269841269841)\n",
      "test (1.0, 0.8)\n",
      "k :  9\n",
      "train (0.9931972789115646, 0.9682539682539683)\n",
      "test (1.0, 1.0)\n",
      "model 1\n",
      "k :  0\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  1\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  2\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  3\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  4\n",
      "train (0.9917355371900827, 0.9583333333333334)\n",
      "test (1.0, 1.0)\n",
      "k :  5\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  6\n",
      "train (1.0, 1.0)\n",
      "test (0.9375, 1.0)\n",
      "k :  7\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  8\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "k :  9\n",
      "train (1.0, 1.0)\n",
      "test (1.0, 1.0)\n",
      "model 2\n",
      "k :  0\n",
      "train (0.9880952380952381, 0.9565217391304348)\n",
      "test (1.0, 0.8571428571428571)\n",
      "k :  1\n",
      "train (1.0, 0.9047619047619048)\n",
      "test (1.0, 1.0)\n",
      "k :  2\n",
      "train (0.9940476190476191, 0.9130434782608695)\n",
      "test (0.975609756097561, 0.8571428571428571)\n",
      "k :  3\n",
      "train (0.9817073170731707, 0.8888888888888888)\n",
      "test (1.0, 1.0)\n",
      "k :  4\n",
      "train (1.0, 0.9615384615384616)\n",
      "test (1.0, 0.75)\n",
      "model 3\n",
      "k :  0\n",
      "train (0.9879518072289156, 0.96)\n",
      "test (1.0, 0.8)\n",
      "k :  1\n",
      "train (1.0, 0.9259259259259259)\n",
      "test (0.9777777777777777, 1.0)\n",
      "k :  2\n",
      "train (1.0, 0.8888888888888888)\n",
      "test (0.9444444444444444, 0.9166666666666666)\n",
      "k :  3\n",
      "train (0.9939024390243902, 0.9259259259259259)\n",
      "test (0.9777777777777777, 1.0)\n",
      "k :  4\n",
      "train (0.9940828402366864, 0.8260869565217391)\n",
      "test (1.0, 0.8571428571428571)\n",
      "model 4\n",
      "k :  0\n",
      "train (1.0, 0.9259259259259259)\n",
      "test (0.9047619047619048, 1.0)\n",
      "k :  1\n",
      "train (1.0, 0.9642857142857143)\n",
      "test (1.0, 0.5)\n",
      "k :  2\n",
      "train (1.0, 0.9310344827586207)\n",
      "test (0.9565217391304348, 1.0)\n",
      "k :  3\n",
      "train (1.0, 0.9285714285714286)\n",
      "test (1.0, 1.0)\n",
      "k :  4\n",
      "train (1.0, 0.9166666666666666)\n",
      "test (1.0, 1.0)\n",
      "k :  5\n",
      "train (1.0, 0.9583333333333334)\n",
      "test (1.0, 0.6666666666666666)\n",
      "k :  6\n",
      "train (1.0, 0.9310344827586207)\n",
      "test (1.0, 1.0)\n",
      "k :  7\n",
      "train (1.0, 0.9285714285714286)\n",
      "test (1.0, 1.0)\n",
      "k :  8\n",
      "train (1.0, 0.9285714285714286)\n",
      "test (1.0, 1.0)\n",
      "k :  9\n",
      "train (1.0, 0.92)\n",
      "test (1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "trains = []\n",
    "tests  = []\n",
    "for i in range(5) :\n",
    "    print(\"model\", i)\n",
    "    k = 10\n",
    "    if i == 2 or i == 3 :\n",
    "        k = 5\n",
    "    trs, tss, _ = k_fold(i, k)\n",
    "    trains.append(trs)\n",
    "    tests.append(tss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = np.array([np.mean(i, axis = 0) for i in trains])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = np.array([np.mean(i, axis = 0) for i in tests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"predicting from crat3f : \", \"predicting crat3f using all the information and then diagnosting (using only all complete data) \",\n",
    "        \"predicting crat3f using all the information and then diagnosting (using all data) : \", \"predicting using data besides crat3f and using genetic programming : \",\n",
    "        \"predicting using data besides crat3f and using neural networks : \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy \n",
      "\n",
      "predicting from crat3f : \n",
      "specificity and sensitivity\n",
      "[0.99427366 0.97049164]\n",
      "predicting crat3f using all the information and then diagnosting (using only all complete data) \n",
      "specificity and sensitivity\n",
      "[0.99917355 0.99583333]\n",
      "predicting crat3f using all the information and then diagnosting (using all data) : \n",
      "specificity and sensitivity\n",
      "[0.99277003 0.92495089]\n",
      "predicting using data besides crat3f and using genetic programming : \n",
      "specificity and sensitivity\n",
      "[0.99518742 0.90536554]\n",
      "predicting using data besides crat3f and using neural networks : \n",
      "specificity and sensitivity\n",
      "[1.         0.93329949]\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy \\n\")\n",
    "for i, iv in enumerate(trains) :\n",
    "    print(names[i])\n",
    "    print(\"specificity and sensitivity\")\n",
    "    print(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy \n",
      "\n",
      "predicting from crat3f : \n",
      "specificity and sensitivity\n",
      "[0.99379071 0.96571429]\n",
      "predicting crat3f using all the information and then diagnosting (using only all complete data) \n",
      "specificity and sensitivity\n",
      "[0.99375 1.     ]\n",
      "predicting crat3f using all the information and then diagnosting (using all data) : \n",
      "specificity and sensitivity\n",
      "[0.99512195 0.89285714]\n",
      "predicting using data besides crat3f and using genetic programming : \n",
      "specificity and sensitivity\n",
      "[0.98      0.9147619]\n",
      "predicting using data besides crat3f and using neural networks : \n",
      "specificity and sensitivity\n",
      "[0.98612836 0.91666667]\n"
     ]
    }
   ],
   "source": [
    "print(\"train accuracy \\n\")\n",
    "for i, iv in enumerate(tests) :\n",
    "    print(names[i])\n",
    "    print(\"specificity and sensitivity\")\n",
    "    print(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
